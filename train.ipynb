{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanazzzmi/Sentiment_analysis_of_the_English_text_with_word_embedding_-_frequency_word/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE1E5N0hSHmE"
      },
      "source": [
        "# import training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGjnF-JcSHmH",
        "outputId": "9b7b3d8b-c803-4442-deec-74987fb580c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578607</th>\n",
              "      <td>1</td>\n",
              "      <td>Zzzzzz.... Finally! Night tweeters!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578608</th>\n",
              "      <td>1</td>\n",
              "      <td>Zzzzzzz, sleep well people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578609</th>\n",
              "      <td>0</td>\n",
              "      <td>ZzzZzZzzzZ... wait no I have homework.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578610</th>\n",
              "      <td>0</td>\n",
              "      <td>ZzZzzzZZZZzzz meh, what am I doing up again?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578611</th>\n",
              "      <td>0</td>\n",
              "      <td>Zzzzzzzzzzzzzzzzzzz, I wish</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1578612 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment                                              tweet\n",
              "0                0                       is so sad for my APL frie...\n",
              "1                0                     I missed the New Moon trail...\n",
              "2                1                            omg its already 7:30 :O\n",
              "3                0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4                0           i think mi bf is cheating on me!!!   ...\n",
              "...            ...                                                ...\n",
              "1578607          1               Zzzzzz.... Finally! Night tweeters! \n",
              "1578608          1                        Zzzzzzz, sleep well people \n",
              "1578609          0            ZzzZzZzzzZ... wait no I have homework. \n",
              "1578610          0      ZzZzzzZZZZzzz meh, what am I doing up again? \n",
              "1578611          0                       Zzzzzzzzzzzzzzzzzzz, I wish \n",
              "\n",
              "[1578612 rows x 2 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "d = 200\n",
        "\n",
        "df = pd.read_csv(\"Sentiment Analysis Dataset.csv\", error_bad_lines=False, warn_bad_lines=False)\n",
        "del df['ItemID']\n",
        "del df['SentimentSource']\n",
        "df.columns = ['sentiment','tweet']\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRlK9pg5SHmJ"
      },
      "source": [
        "# preprocessing training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmJimX4WSHmK"
      },
      "outputs": [],
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \" you \",\n",
        "\" ur \": \" your \",\n",
        "\" n \": \" and \"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WauNcCvSHmL",
        "outputId": "626794ee-2e2f-4b6c-a8a1-480a497d339c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                                is so sad for my apl friend\n",
              "1                              i missed the new moon trailer\n",
              "2                                          omg its already o\n",
              "3          omgaga im sooo im gunna cry i have been at thi...\n",
              "4                         i think mi bf is cheating on me tt\n",
              "                                 ...                        \n",
              "1578607                        zzzzzz finally night tweeters\n",
              "1578608                            zzzzzzz sleep well people\n",
              "1578609                   zzzzzzzzzz wait no i have homework\n",
              "1578610           zzzzzzzzzzzzz meh what am i doing up again\n",
              "1578611                           zzzzzzzzzzzzzzzzzzz i wish\n",
              "Name: tweet, Length: 1578612, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = ' '.join(df['tweet'])\n",
        "text = text.split()\n",
        "freq_comm = pd.Series(text).value_counts()\n",
        "rare = freq_comm[freq_comm.values == 1]\n",
        "\n",
        "def get_clean_text(x):\n",
        "    if type(x) is str:\n",
        "        x = x.lower()\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        x = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x) \n",
        "        #regex to remove to emails\n",
        "        x = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)\n",
        "        #regex to remove URLs\n",
        "        x = re.sub('RT', \"\", x)\n",
        "        #substitute the 'RT' retweet tags with empty spaces\n",
        "        x = re.sub('[^A-Z a-z]+', '', x)\n",
        "        #combining all the text excluding rare words.\n",
        "        x = ' '.join([t for t in x.split() if t not in rare])\n",
        "        return x\n",
        "    else:\n",
        "        return x\n",
        "    \n",
        "df['tweet'] = df['tweet'].apply(lambda x: get_clean_text(x))\n",
        "df['tweet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94JJ3t-7SHmL",
        "outputId": "a0cffabe-0bb6-48d7-c31f-062085a92594"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    790177\n",
              "0    788435\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmnSXXWFSHmM"
      },
      "outputs": [],
      "source": [
        "text = df['tweet'].tolist()\n",
        "y = df['sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uK1KGWZSHmM"
      },
      "source": [
        "# Each word has an id\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(text)\n",
        "#token.word_index.items()"
      ],
      "metadata": {
        "id": "43mUQRJ-S5dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrwRNIyLSHmN",
        "outputId": "ba38a598-fb27-4c9d-db15-48e14493157c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "642315"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size  = len(token.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f96LoGe7SHmO"
      },
      "source": [
        "# Each sentence is written according to the number of the word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5llhgHAdSHmO",
        "outputId": "225073b8-a9c3-4174-bc49-8d3901eab7d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5, 19, 105, 11, 7, 217244, 246], [1, 237, 3, 71, 763, 1232], [217, 66, 188, 419]]\n"
          ]
        }
      ],
      "source": [
        "encoded_text = token.texts_to_sequences(text)\n",
        "print(encoded_text[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt1Ff1IbSHmO",
        "outputId": "35fb1617-4f80-41ae-a8d1-ff06ac1dd79b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[     5     19    105 ...      0      0      0]\n",
            " [     1    237      3 ...      0      0      0]\n",
            " [   217     66    188 ...      0      0      0]\n",
            " ...\n",
            " [ 24800    133     39 ...      0      0      0]\n",
            " [ 48716   1985     52 ...      0      0      0]\n",
            " [134663      1    108 ...      0      0      0]]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = 120\n",
        "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahky1PurSHmP"
      },
      "source": [
        "# use word embedding glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NInLcpUQSHmP",
        "outputId": "bfe59649-490b-4558-968a-37fb6635a479"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1193515"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "glove_vectors = dict()\n",
        "# file = open('glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "file = open('glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #storing the word in the variable\n",
        "    vectors = np.asarray(values[1: ])\n",
        "    #storing the vector representation of the respective word in the dictionary\n",
        "    glove_vectors[word] = vectors\n",
        "file.close()\n",
        "len(glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iDi_hLDSHmP",
        "outputId": "16367583-191f-47ed-8d52-245a2c600d36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['0.1964', '0.67153', '0.0062976', '0.25359', '-0.42097', '0.3849',\n",
              "       '1.0378', '-0.18536', '-0.054244', '-0.10856', '0.12146',\n",
              "       '0.047692', '-0.93228', '-0.27192', '-0.3506', '0.11069',\n",
              "       '-0.055099', '-0.079658', '0.011767', '0.14395', '-0.025917',\n",
              "       '-0.18253', '0.025691', '0.19619', '0.10334', '1.0731', '0.41992',\n",
              "       '0.17083', '0.91596', '-0.43398', '0.16464', '0.012715',\n",
              "       '0.095477', '0.1449', '-0.29652', '-0.10642', '0.098389',\n",
              "       '0.17937', '0.23289', '-0.22209', '0.08777', '-0.18005', '0.38678',\n",
              "       '-0.032059', '0.1202', '-0.41741', '0.26827', '-0.19769',\n",
              "       '0.021976', '-0.23585', '0.14789', '0.10173', '-0.1038',\n",
              "       '-0.31954', '0.63308', '-0.051142', '-0.053209', '0.29378',\n",
              "       '-0.054395', '4.3346e-05', '0.25585', '-0.18048', '-0.2703',\n",
              "       '0.0077783', '0.63585', '0.14869', '-0.10854', '-0.24726',\n",
              "       '-0.10869', '-0.24147', '0.28059', '0.13645', '-0.03561',\n",
              "       '-0.68258', '0.11957', '0.011085', '-0.013771', '-0.17603',\n",
              "       '-0.57826', '-0.041571', '-0.12342', '0.58091', '-0.044446',\n",
              "       '0.14833', '-0.081576', '-0.15804', '-0.43868', '-0.090296',\n",
              "       '-0.27891', '-0.032887', '0.11688', '0.054837', '0.10082',\n",
              "       '0.24519', '0.15062', '0.038064', '-0.25851', '-0.23446',\n",
              "       '0.064198', '0.12255', '-0.52537', '0.3335', '-0.23323', '-0.2578',\n",
              "       '0.29035', '0.13483', '0.065876', '0.38322', '0.11237',\n",
              "       '-0.0023038', '-0.28184', '0.17724', '0.56759', '0.24386',\n",
              "       '-0.040227', '-0.045024', '-0.34185', '-0.41455', '-0.04111',\n",
              "       '0.22438', '-0.11626', '0.20744', '-0.11859', '-0.077585',\n",
              "       '0.33058', '-0.28058', '0.049034', '-0.19482', '0.067283',\n",
              "       '0.03007', '0.069742', '-0.13861', '0.62041', '0.38005',\n",
              "       '-0.050533', '0.097378', '0.27874', '-0.17388', '0.37021',\n",
              "       '-0.66593', '0.14769', '-0.063142', '-0.12092', '0.37639',\n",
              "       '-0.20316', '0.36495', '0.25422', '0.16774', '0.050256',\n",
              "       '0.054627', '0.10653', '-0.070859', '-6.7986', '0.40561',\n",
              "       '-0.38071', '0.32075', '-0.33545', '-0.015599', '-0.46192',\n",
              "       '0.2986', '0.51756', '-0.17136', '-0.83454', '-0.14496',\n",
              "       '-0.36553', '-0.18636', '0.10103', '0.36616', '0.1311', '0.51864',\n",
              "       '-0.41521', '-0.1965', '-0.006874', '-0.014722', '-0.33198',\n",
              "       '0.13165', '0.29492', '-0.19796', '-0.28374', '-0.14532',\n",
              "       '0.19547', '0.11138', '0.12032', '0.21849', '0.47585', '-0.78959',\n",
              "       '0.3075', '0.010715', '-0.21146', '-0.020619', '-0.11518',\n",
              "       '0.55363', '-0.185', '-0.19653', '0.10566', '-0.26214', '0.20397',\n",
              "       '-0.30815', '-0.12312', '0.048926'], dtype='<U10')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_vectors.get('you')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtrVkJMNSHmP"
      },
      "outputs": [],
      "source": [
        "glove_vectors.get('yogdfgfdgdgu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey2X1gKPSHmQ"
      },
      "source": [
        "#Find words in word embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_matrix = np.zeros((vocab_size, d))\n",
        "\n",
        "\n",
        "not_word = []\n",
        "have_word = []\n",
        "\n",
        "for word, index in token.word_index.items()  :# Ù‡Ø± Ú©Ù„Ù…Ù‡ ÛŒÙ‡ Ø´Ù†Ø§Ø³Ù‡ Ø¯Ø§Ø±Ù‡\n",
        "    vector = glove_vectors.get(word)\n",
        "    if vector is not None:\n",
        "        word_vector_matrix[index] = vector\n",
        "        have_word.append(word)\n",
        "    else:\n",
        "        not_word.append(word)\n",
        "    \n",
        "#have_word =>Words in word embedding"
      ],
      "metadata": {
        "id": "rg7NNCt6ufIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4OVJNw-SHmQ"
      },
      "source": [
        "#How many words are seen in positive sentences and how many in negative sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG0x_4wKSHmQ",
        "outputId": "834c1d26-e11f-4e9e-ef83-8d1c0989bbbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>lable</th>\n",
              "      <th>frequent_negative_sentence</th>\n",
              "      <th>frequent_positive_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i</td>\n",
              "      <td>0</td>\n",
              "      <td>556579</td>\n",
              "      <td>369736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to</td>\n",
              "      <td>0</td>\n",
              "      <td>310717</td>\n",
              "      <td>249686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>1</td>\n",
              "      <td>255847</td>\n",
              "      <td>262505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a</td>\n",
              "      <td>1</td>\n",
              "      <td>179817</td>\n",
              "      <td>195778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>186404</td>\n",
              "      <td>164141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83501</th>\n",
              "      <td>melantai</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83502</th>\n",
              "      <td>kellies</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83503</th>\n",
              "      <td>hanggat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83504</th>\n",
              "      <td>zaynah</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83505</th>\n",
              "      <td>zegna</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100198 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word  lable  frequent_negative_sentence  frequent_positive_sentence\n",
              "0             i      0                      556579                      369736\n",
              "1            to      0                      310717                      249686\n",
              "2           the      1                      255847                      262505\n",
              "3             a      1                      179817                      195778\n",
              "4            is      0                      186404                      164141\n",
              "...         ...    ...                         ...                         ...\n",
              "83501  melantai      1                           0                           1\n",
              "83502   kellies      1                           0                           1\n",
              "83503   hanggat      1                           0                           1\n",
              "83504    zaynah      1                           0                           1\n",
              "83505     zegna      1                           0                           1\n",
              "\n",
              "[100198 rows x 4 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Divide tweets into tags of one and tags of zeros\n",
        "df_y = pd.DataFrame(np.array(y),columns=['a'])\n",
        "df_one = df_y[df_y['a']==1].index.values.astype(int)\n",
        "df_zero = df_y[df_y['a']==0].index.values.astype(int)\n",
        "\n",
        "text_one = []\n",
        "for x in df_one:\n",
        "    text_one.append(text[x])\n",
        "\n",
        "text_zero = []\n",
        "for x in df_zero:\n",
        "    text_zero.append(text[x])\n",
        "\n",
        "join_text_zero = [' '.join(text_zero)]\n",
        "join_text_one = [' '.join(text_one)]\n",
        "\n",
        "df_join_text_zero = pd.DataFrame(join_text_zero[0:],columns= [\"tex\"])\n",
        "df_join_text_one = pd.DataFrame(join_text_one[0:],columns= [\"tex\"])\n",
        "\n",
        "df_have_word = pd.DataFrame({'word':have_word})\n",
        "df_token = pd.DataFrame.from_dict(token.word_index.items())\n",
        "\n",
        "df_text_one = pd.DataFrame(text_one[0:],columns= [\"tex\"])\n",
        "df_text_zero = pd.DataFrame(text_zero[0:],columns= [\"tex\"])\n",
        "\n",
        "from collections import Counter\n",
        "count_zero = df_join_text_zero['tex'].str.split().apply(Counter)\n",
        "count_one = df_join_text_one['tex'].str.split().apply(Counter)\n",
        "\n",
        "df_counter_zero = pd.DataFrame.from_dict(count_zero[0], orient='index').reset_index()\n",
        "df_counter_one = pd.DataFrame.from_dict(count_one[0], orient='index').reset_index()\n",
        "df_counter_zero.columns = ['word', 'value']\n",
        "df_counter_one.columns = ['word', 'value']\n",
        "\n",
        "df_token.columns = ['word', 'id']\n",
        "\n",
        "sentiment_zero =df_have_word.merge(df_counter_zero, left_on='word', right_on='word')[['value','word']]\n",
        "sentiment_one = df_have_word.merge(df_counter_one, left_on='word', right_on='word')[['value','word']]\n",
        "\n",
        "#Words found in both positive and negative sentences\n",
        "common = sentiment_one.merge(sentiment_zero,on=['word','word'])\n",
        "common.columns = ['value_one', 'word' , 'value_zero']\n",
        "\n",
        "common[\"max\"] = common[[\"value_one\", \"value_zero\"]].max(axis=1)\n",
        "common[\"min\"] = common[[\"value_one\", \"value_zero\"]].min(axis=1)\n",
        "\n",
        "#Words that were both positive and negative in the sentence were marked with a label column that were more commonly seen in positive or negative sentences.\n",
        "common[\"lable\"] = np.where(common[\"value_one\"] == common[\"max\"], 1, 0)\n",
        "common[\"frequent_negative_sentence\"] = np.where(common[\"lable\"] == 1 , common[\"min\"], common[\"max\"])\n",
        "common[\"frequent_positive_sentence\"] = np.where(common[\"lable\"] == 0 , common[\"min\"], common[\"max\"])\n",
        "\n",
        "common_df_final = common.drop('value_one', 1)\n",
        "common_df_final = common_df_final.drop('value_zero', 1)\n",
        "common_df_final = common_df_final.drop('max', 1)\n",
        "common_df_final = common_df_final.drop('min', 1)\n",
        "\n",
        "#Those words that are seen in positive sentences and not seen in negative sentences\n",
        "just_one = sentiment_one[(~sentiment_one.word.isin(common.word))&(~sentiment_one.word.isin(common.word))]\n",
        "just_one = just_one.assign(lable = 1)\n",
        "just_one = just_one.assign(frequent_negative_sentence = 0)\n",
        "just_one.columns = ['frequent_positive_sentence', 'word' , 'lable','frequent_negative_sentence']\n",
        "\n",
        "#Those words that are seen in negative sentences and not seen in positive sentences\n",
        "just_zero = sentiment_zero[(~sentiment_zero.word.isin(common.word))&(~sentiment_zero.word.isin(common.word))]\n",
        "just_zero = just_zero.assign(lable = 0)\n",
        "just_zero = just_zero.assign(frequent_posoitve_sentence = 0)\n",
        "just_zero.columns = ['frequent_negative_sentence', 'word' , 'lable','frequent_positive_sentence']\n",
        "\n",
        "frames = [common_df_final,just_zero, just_one]\n",
        "result = pd.concat(frames)\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtS-WGgGSHmR"
      },
      "source": [
        "# WI calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0moOLodSHmR",
        "outputId": "0fbad99e-6c4b-48e4-9fdb-b697c9393f9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>lable</th>\n",
              "      <th>frequent_negative_sentence</th>\n",
              "      <th>frequent_positive_sentence</th>\n",
              "      <th>WI_positive_word</th>\n",
              "      <th>WI_negative_word</th>\n",
              "      <th>WI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i</td>\n",
              "      <td>0</td>\n",
              "      <td>556579</td>\n",
              "      <td>369736</td>\n",
              "      <td>None</td>\n",
              "      <td>0.335699</td>\n",
              "      <td>0.335699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to</td>\n",
              "      <td>0</td>\n",
              "      <td>310717</td>\n",
              "      <td>249686</td>\n",
              "      <td>None</td>\n",
              "      <td>0.19642</td>\n",
              "      <td>0.196420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>1</td>\n",
              "      <td>255847</td>\n",
              "      <td>262505</td>\n",
              "      <td>0.025363</td>\n",
              "      <td>None</td>\n",
              "      <td>0.025363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a</td>\n",
              "      <td>1</td>\n",
              "      <td>179817</td>\n",
              "      <td>195778</td>\n",
              "      <td>0.081526</td>\n",
              "      <td>None</td>\n",
              "      <td>0.081526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is</td>\n",
              "      <td>0</td>\n",
              "      <td>186404</td>\n",
              "      <td>164141</td>\n",
              "      <td>None</td>\n",
              "      <td>0.119434</td>\n",
              "      <td>0.119434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83501</th>\n",
              "      <td>melantai</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83502</th>\n",
              "      <td>kellies</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83503</th>\n",
              "      <td>hanggat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83504</th>\n",
              "      <td>zaynah</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83505</th>\n",
              "      <td>zegna</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100198 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word  lable  frequent_negative_sentence  \\\n",
              "0             i      0                      556579   \n",
              "1            to      0                      310717   \n",
              "2           the      1                      255847   \n",
              "3             a      1                      179817   \n",
              "4            is      0                      186404   \n",
              "...         ...    ...                         ...   \n",
              "83501  melantai      1                           0   \n",
              "83502   kellies      1                           0   \n",
              "83503   hanggat      1                           0   \n",
              "83504    zaynah      1                           0   \n",
              "83505     zegna      1                           0   \n",
              "\n",
              "       frequent_positive_sentence WI_positive_word WI_negative_word        WI  \n",
              "0                          369736             None         0.335699  0.335699  \n",
              "1                          249686             None          0.19642  0.196420  \n",
              "2                          262505         0.025363             None  0.025363  \n",
              "3                          195778         0.081526             None  0.081526  \n",
              "4                          164141             None         0.119434  0.119434  \n",
              "...                           ...              ...              ...       ...  \n",
              "83501                           1              1.0             None  1.000000  \n",
              "83502                           1              1.0             None  1.000000  \n",
              "83503                           1              1.0             None  1.000000  \n",
              "83504                           1              1.0             None  1.000000  \n",
              "83505                           1              1.0             None  1.000000  \n",
              "\n",
              "[100198 rows x 7 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['WI_positive_word'] = np.where(result['lable']== 1 , 1- result['frequent_negative_sentence']/ result['frequent_positive_sentence'], None)\n",
        "result['WI_negative_word'] = np.where(result['lable']== 0 , 1- result['frequent_positive_sentence']/ result['frequent_negative_sentence'], None)\n",
        "result['WI'] = result['WI_positive_word'].fillna(result['WI_negative_word'])\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9uYMnmtSHmR"
      },
      "source": [
        "# Find words in the sentence that are in word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlM0ng5ASHmR"
      },
      "outputs": [],
      "source": [
        "word_have_list= list(result.word)\n",
        "word_have_set = set(word_have_list)\n",
        "result_word_list = result['word'].tolist()\n",
        "result_WI_list = result['WI'].tolist()\n",
        "\n",
        "def word_WI_finder(x):\n",
        "    df_words = set(x.split(' '))\n",
        "    extract_words =  word_set.intersection(df_words)\n",
        "    #index_word = result_word_list.index(extract_words)\n",
        "    \n",
        "    \n",
        "    return (extract_words)\n",
        "\n",
        "\n",
        "word_set = word_have_set\n",
        "\n",
        "df['ddd_WI'] = df.tweet.apply(word_WI_finder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfoBXEySHmS"
      },
      "source": [
        "# Calculate weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "rwhozkuVSHmS",
        "outputId": "3fc612c2-3035-4d97-b9f3-a77030e8451f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.11417525396010611,\n",
              " 0.04050639402398644,\n",
              " 0.08692084970868079,\n",
              " 0.33915257924121495,\n",
              " 0.04535108576770854,\n",
              " 0.05235419367581135,\n",
              " 0.32153964362249177]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WI_ww = []\n",
        "for x in df['ddd_WI'][:]:\n",
        "    WI_w = []\n",
        "    for y in x : \n",
        "        ind = result_word_list.index(y)\n",
        "        WI_w.append(result_WI_list[ind])\n",
        "    WI_ww.append(WI_w)\n",
        "    \n",
        "WI_ww_new = [[] if x==[0] else x for x in WI_ww]\n",
        "WI_ww_neww = [[] if x==[0,0] else x for x in WI_ww_new]\n",
        "Weight_sentence = [[float(j)/sum(i) for j in i] for i in WI_ww_neww[:]]\n",
        "\n",
        "Weight_sentence[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiVwAC8FSHmS"
      },
      "source": [
        "# Calculate the final sentence vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poSwiD5uSHmS"
      },
      "outputs": [],
      "source": [
        "#part 1 :  start _ 789305"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq0AruOcSHmS",
        "outputId": "d04ccff4-a468-4188-c2d4-cf75541dcc79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_ss1 = []\n",
        "for i in range(len(df['ddd_WI'][0 : int(len(df)/2)])):\n",
        "    #print(df['ddd_WI'][x])\n",
        "    vv = []\n",
        "    for j in range(len(df['ddd_WI'][i])):\n",
        "        #print(list(df['ddd_WI'][i])[j])\n",
        "        if len(Weight_sentence[i]) > 1:\n",
        "            v = []\n",
        "            for num in range(d):\n",
        "                vec = Weight_sentence[i][j] * float(glove_vectors.get(list(df['ddd_WI'][i])[j])[num])\n",
        "                v.append(vec)\n",
        "            #print(v)\n",
        "            vv.append(v)\n",
        "        #print(vv)\n",
        "        else:\n",
        "            vv.append(glove_vectors.get(list(df['ddd_WI'][i])[j]).astype(np.float))\n",
        "            \n",
        "    s = [sum(x) for x in zip(*vv)]\n",
        "    sentence_ss1.append(s)\n",
        "\n",
        "\n",
        "data1 = pd.DataFrame(sentence_ss1[0:] , columns = list(range(d)))\n",
        "\n",
        "data1[:].to_csv('X_train1.csv', sep='\\t', encoding='utf-8')\n",
        "df['sentiment'][0 : int(len(df)/2)].to_csv('y_train1.csv', sep='\\t', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdzb8GqVSHmU",
        "outputId": "fd21fc56-f585-46ba-eb39-2bee286c026b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#part 2 : 789306 _ 1578612\n",
        "sentence_ss2 = []\n",
        "for i in range(int(len(df)/2) , int(len(df))):\n",
        "    #print(df['ddd_WI'][x])\n",
        "    vv = []\n",
        "    for j in range(len(df['ddd_WI'][i])):\n",
        "        #print(list(df['ddd_WI'][i])[j])\n",
        "        if len(Weight_sentence[i]) > 1:\n",
        "            v = []\n",
        "            for num in range(d):\n",
        "                vec = Weight_sentence[i][j] * float(glove_vectors.get(list(df['ddd_WI'][i])[j])[num])\n",
        "                v.append(vec)\n",
        "            #print(v)\n",
        "            vv.append(v)\n",
        "        #print(vv)\n",
        "        else:\n",
        "            vv.append(glove_vectors.get(list(df['ddd_WI'][i])[j]).astype(np.float))\n",
        "            \n",
        "    s = [sum(x) for x in zip(*vv)]\n",
        "    sentence_ss2.append(s)\n",
        "\n",
        "\n",
        "data2 = pd.DataFrame(sentence_ss2[0:] , columns = list(range(d)))\n",
        "\n",
        "data2[:].to_csv('X_train2.csv', sep='\\t', encoding='utf-8')\n",
        "df['sentiment'][int(len(df)/2) : ].to_csv('y_train2.csv', sep='\\t', encoding='utf-8')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jE1E5N0hSHmE",
        "HRlK9pg5SHmJ",
        "6uK1KGWZSHmM",
        "f96LoGe7SHmO",
        "ahky1PurSHmP",
        "K4OVJNw-SHmQ",
        "YtS-WGgGSHmR",
        "L9uYMnmtSHmR",
        "bbfoBXEySHmS",
        "YiVwAC8FSHmS"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}