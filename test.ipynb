{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanazzzmi/Sentiment_analysis_of_the_English_text_with_word_embedding_-_frequency_word/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okFLPgIUwSwE"
      },
      "source": [
        "# import testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i9-hvhlwSwK",
        "outputId": "1f5bc143-c37b-4d4a-fd80-27e8ee561752"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1467933112</td>\n",
              "      <td>0</td>\n",
              "      <td>the angel is going to miss the athlete this we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2323395086</td>\n",
              "      <td>0</td>\n",
              "      <td>It looks as though Shaq is getting traded to C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1467968979</td>\n",
              "      <td>0</td>\n",
              "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1990283756</td>\n",
              "      <td>0</td>\n",
              "      <td>drinking a McDonalds coffee and not understand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1988884918</td>\n",
              "      <td>0</td>\n",
              "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2029</th>\n",
              "      <td>1468049681</td>\n",
              "      <td>0</td>\n",
              "      <td>A king sized bed is nice but sad and lonely wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2030</th>\n",
              "      <td>2195475499</td>\n",
              "      <td>0</td>\n",
              "      <td>@breannalovesjb hurry up home!!!!! im dying wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2031</th>\n",
              "      <td>1996172176</td>\n",
              "      <td>0</td>\n",
              "      <td>@jordanhowell lol only a PSP, had a game boy b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2032</th>\n",
              "      <td>2016105580</td>\n",
              "      <td>1</td>\n",
              "      <td>Good morning everyone!  It is such a beautiful...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2033</th>\n",
              "      <td>2186977170</td>\n",
              "      <td>1</td>\n",
              "      <td>hey guess was @magicmanil the Lakers won and K...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2034 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  sentiment                                              tweet\n",
              "0     1467933112          0  the angel is going to miss the athlete this we...\n",
              "1     2323395086          0  It looks as though Shaq is getting traded to C...\n",
              "2     1467968979          0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \n",
              "3     1990283756          0  drinking a McDonalds coffee and not understand...\n",
              "4     1988884918          0  So dissapointed Taylor Swift doesnt have a Twi...\n",
              "...          ...        ...                                                ...\n",
              "2029  1468049681          0  A king sized bed is nice but sad and lonely wi...\n",
              "2030  2195475499          0  @breannalovesjb hurry up home!!!!! im dying wi...\n",
              "2031  1996172176          0  @jordanhowell lol only a PSP, had a game boy b...\n",
              "2032  2016105580          1  Good morning everyone!  It is such a beautiful...\n",
              "2033  2186977170          1  hey guess was @magicmanil the Lakers won and K...\n",
              "\n",
              "[2034 rows x 3 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "d = 200\n",
        "\n",
        "df = pd.read_csv(\"STG.txt\",sep=';')\n",
        "df.columns = ['id','sentiment','tweet']\n",
        "df[\"sentiment\"].replace({4: 1}, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqJIfulAwSwL"
      },
      "source": [
        "# preprocessing testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp6txn4CwSwM"
      },
      "outputs": [],
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \" you \",\n",
        "\" ur \": \" your \",\n",
        "\" n \": \" and \"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHEUXGjEwSwM",
        "outputId": "6aa48a3e-bf5c-4418-e6cc-e603248af322"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0             the angel is going to miss the this weekend\n",
              "1       it looks as though shaq is getting to clevelan...\n",
              "2            clarianne april th is not coming soon enough\n",
              "3       drinking a mcdonalds coffee and not why someon...\n",
              "4                   so taylor swift doesnt have a twitter\n",
              "                              ...                        \n",
              "2029    a king bed is nice but sad and lonely with no ...\n",
              "2030    breannalovesjb hurry up home im dying with no ...\n",
              "2031    jordanhowell lol only a psp had a game boy but...\n",
              "2032    good morning everyone it is such a beautiful d...\n",
              "2033    hey guess was magicmanil the lakers won and ko...\n",
              "Name: tweet, Length: 2034, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = ' '.join(df['tweet'])\n",
        "text = text.split()\n",
        "freq_comm = pd.Series(text).value_counts()\n",
        "rare = freq_comm[freq_comm.values == 1]\n",
        "\n",
        "def get_clean_text(x):\n",
        "    if type(x) is str:\n",
        "        x = x.lower()\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        x = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x) \n",
        "        #regex to remove to emails\n",
        "        x = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)\n",
        "        #regex to remove URLs\n",
        "        x = re.sub('RT', \"\", x)\n",
        "        #substitute the 'RT' retweet tags with empty spaces\n",
        "        x = re.sub('[^A-Z a-z]+', '', x)\n",
        "        #combining all the text excluding rare words.\n",
        "        x = ' '.join([t for t in x.split() if t not in rare])\n",
        "        return x\n",
        "    else:\n",
        "        return x\n",
        "    \n",
        "df['tweet'] = df['tweet'].apply(lambda x: get_clean_text(x))\n",
        "df['tweet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE4jtDjSwSwN",
        "outputId": "61286d1e-d55a-4a8e-a358-e18161b7222d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1402\n",
              "1     632\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXbwCgzCwSwN"
      },
      "outputs": [],
      "source": [
        "text = df['tweet'].tolist()\n",
        "y = df['sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AThT9n-ZwSwN"
      },
      "source": [
        "# Each word has an id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(text)\n",
        "#token.word_index.items()"
      ],
      "metadata": {
        "id": "LjDJ4kDJwmFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT9_72C3wSwO",
        "outputId": "3391bf96-6274-4536-a3c0-d1636daf5ddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3637"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size  = len(token.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TJc6F38wSwO"
      },
      "source": [
        "# Each sentence is written according to the number of the word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cmJlWBZwSwP",
        "outputId": "830045e7-f244-4054-cd23-0215fd6c4984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3, 851, 6, 37, 2, 77, 3, 26, 248], [9, 274, 97, 180, 716, 6, 203, 2, 717, 2, 183, 89, 46, 132, 10, 1062, 476, 3, 230, 1505, 6, 34, 92], [1506, 1063, 258, 6, 12, 311, 259, 362]]\n"
          ]
        }
      ],
      "source": [
        "encoded_text = token.texts_to_sequences(text)\n",
        "print(encoded_text[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVuDzcS9wSwQ",
        "outputId": "b4e895e2-99d2-4818-cc5d-0ec98e350633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   3  851    6 ...    0    0    0]\n",
            " [   9  274   97 ...    0    0    0]\n",
            " [1506 1063  258 ...    0    0    0]\n",
            " ...\n",
            " [3635   96  159 ...    0    0    0]\n",
            " [  40  138  284 ...    0    0    0]\n",
            " [ 190  197   22 ...    0    0    0]]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_length = 120\n",
        "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9c-ETSSwSwQ"
      },
      "source": [
        "# use word embedding glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yha8tRj8wSwQ",
        "outputId": "877277bf-1f7d-4c68-d22f-0ab5b3cba8e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1193515"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "glove_vectors = dict()\n",
        "# file = open('glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "file = open('glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #storing the word in the variable\n",
        "    vectors = np.asarray(values[1: ])\n",
        "    #storing the vector representation of the respective word in the dictionary\n",
        "    glove_vectors[word] = vectors\n",
        "file.close()\n",
        "len(glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQS-evWbwSwQ",
        "outputId": "e9305a00-9786-441c-9042-d8604d35d0cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['0.1964', '0.67153', '0.0062976', '0.25359', '-0.42097', '0.3849',\n",
              "       '1.0378', '-0.18536', '-0.054244', '-0.10856', '0.12146',\n",
              "       '0.047692', '-0.93228', '-0.27192', '-0.3506', '0.11069',\n",
              "       '-0.055099', '-0.079658', '0.011767', '0.14395', '-0.025917',\n",
              "       '-0.18253', '0.025691', '0.19619', '0.10334', '1.0731', '0.41992',\n",
              "       '0.17083', '0.91596', '-0.43398', '0.16464', '0.012715',\n",
              "       '0.095477', '0.1449', '-0.29652', '-0.10642', '0.098389',\n",
              "       '0.17937', '0.23289', '-0.22209', '0.08777', '-0.18005', '0.38678',\n",
              "       '-0.032059', '0.1202', '-0.41741', '0.26827', '-0.19769',\n",
              "       '0.021976', '-0.23585', '0.14789', '0.10173', '-0.1038',\n",
              "       '-0.31954', '0.63308', '-0.051142', '-0.053209', '0.29378',\n",
              "       '-0.054395', '4.3346e-05', '0.25585', '-0.18048', '-0.2703',\n",
              "       '0.0077783', '0.63585', '0.14869', '-0.10854', '-0.24726',\n",
              "       '-0.10869', '-0.24147', '0.28059', '0.13645', '-0.03561',\n",
              "       '-0.68258', '0.11957', '0.011085', '-0.013771', '-0.17603',\n",
              "       '-0.57826', '-0.041571', '-0.12342', '0.58091', '-0.044446',\n",
              "       '0.14833', '-0.081576', '-0.15804', '-0.43868', '-0.090296',\n",
              "       '-0.27891', '-0.032887', '0.11688', '0.054837', '0.10082',\n",
              "       '0.24519', '0.15062', '0.038064', '-0.25851', '-0.23446',\n",
              "       '0.064198', '0.12255', '-0.52537', '0.3335', '-0.23323', '-0.2578',\n",
              "       '0.29035', '0.13483', '0.065876', '0.38322', '0.11237',\n",
              "       '-0.0023038', '-0.28184', '0.17724', '0.56759', '0.24386',\n",
              "       '-0.040227', '-0.045024', '-0.34185', '-0.41455', '-0.04111',\n",
              "       '0.22438', '-0.11626', '0.20744', '-0.11859', '-0.077585',\n",
              "       '0.33058', '-0.28058', '0.049034', '-0.19482', '0.067283',\n",
              "       '0.03007', '0.069742', '-0.13861', '0.62041', '0.38005',\n",
              "       '-0.050533', '0.097378', '0.27874', '-0.17388', '0.37021',\n",
              "       '-0.66593', '0.14769', '-0.063142', '-0.12092', '0.37639',\n",
              "       '-0.20316', '0.36495', '0.25422', '0.16774', '0.050256',\n",
              "       '0.054627', '0.10653', '-0.070859', '-6.7986', '0.40561',\n",
              "       '-0.38071', '0.32075', '-0.33545', '-0.015599', '-0.46192',\n",
              "       '0.2986', '0.51756', '-0.17136', '-0.83454', '-0.14496',\n",
              "       '-0.36553', '-0.18636', '0.10103', '0.36616', '0.1311', '0.51864',\n",
              "       '-0.41521', '-0.1965', '-0.006874', '-0.014722', '-0.33198',\n",
              "       '0.13165', '0.29492', '-0.19796', '-0.28374', '-0.14532',\n",
              "       '0.19547', '0.11138', '0.12032', '0.21849', '0.47585', '-0.78959',\n",
              "       '0.3075', '0.010715', '-0.21146', '-0.020619', '-0.11518',\n",
              "       '0.55363', '-0.185', '-0.19653', '0.10566', '-0.26214', '0.20397',\n",
              "       '-0.30815', '-0.12312', '0.048926'], dtype='<U10')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_vectors.get('you')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tNSqeycwSwR"
      },
      "outputs": [],
      "source": [
        "glove_vectors.get('yogdfgfdgdgu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KInDZ1ytwSwR"
      },
      "source": [
        "# Find words in word embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_matrix = np.zeros((vocab_size, d))\n",
        "\n",
        "\n",
        "not_word = []\n",
        "have_word = []\n",
        "\n",
        "for word, index in token.word_index.items()  :# هر کلمه یه شناسه داره\n",
        "    vector = glove_vectors.get(word)\n",
        "    if vector is not None:\n",
        "        word_vector_matrix[index] = vector\n",
        "        have_word.append(word)\n",
        "    else:\n",
        "        not_word.append(word)\n",
        "        \n",
        "        \n",
        "#have_word =>Words in word embedding"
      ],
      "metadata": {
        "id": "GdgKP9qTwz6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b91cMd8xwSwR"
      },
      "source": [
        "# How many words are seen in positive sentences and how many in negative sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X9jkPyMwSwR",
        "outputId": "0c46a9f1-e001-4304-b9ed-e23935417551"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>lable</th>\n",
              "      <th>frequent_negative_sentence</th>\n",
              "      <th>frequent_positive_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i</td>\n",
              "      <td>0</td>\n",
              "      <td>980</td>\n",
              "      <td>325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to</td>\n",
              "      <td>0</td>\n",
              "      <td>582</td>\n",
              "      <td>261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>503</td>\n",
              "      <td>253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a</td>\n",
              "      <td>0</td>\n",
              "      <td>364</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my</td>\n",
              "      <td>0</td>\n",
              "      <td>404</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>whithout</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>notebook</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>coincidence</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1406</th>\n",
              "      <td>gsoc</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>cafe</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2557 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             word  lable  frequent_negative_sentence  \\\n",
              "0               i      0                         980   \n",
              "1              to      0                         582   \n",
              "2             the      0                         503   \n",
              "3               a      0                         364   \n",
              "4              my      0                         404   \n",
              "...           ...    ...                         ...   \n",
              "1403     whithout      1                           0   \n",
              "1404     notebook      1                           0   \n",
              "1405  coincidence      1                           0   \n",
              "1406         gsoc      1                           0   \n",
              "1407         cafe      1                           0   \n",
              "\n",
              "      frequent_positive_sentence  \n",
              "0                            325  \n",
              "1                            261  \n",
              "2                            253  \n",
              "3                            163  \n",
              "4                            113  \n",
              "...                          ...  \n",
              "1403                           1  \n",
              "1404                           1  \n",
              "1405                           1  \n",
              "1406                           1  \n",
              "1407                           1  \n",
              "\n",
              "[2557 rows x 4 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Divide tweets into tags of one and tags of zeros\n",
        "df_y = pd.DataFrame(np.array(y),columns=['a'])\n",
        "df_one = df_y[df_y['a']==1].index.values.astype(int)\n",
        "df_zero = df_y[df_y['a']==0].index.values.astype(int)\n",
        "\n",
        "text_one = []\n",
        "for x in df_one:\n",
        "    text_one.append(text[x])\n",
        "\n",
        "text_zero = []\n",
        "for x in df_zero:\n",
        "    text_zero.append(text[x])\n",
        "\n",
        "join_text_zero = [' '.join(text_zero)]\n",
        "join_text_one = [' '.join(text_one)]\n",
        "\n",
        "df_join_text_zero = pd.DataFrame(join_text_zero[0:],columns= [\"tex\"])\n",
        "df_join_text_one = pd.DataFrame(join_text_one[0:],columns= [\"tex\"])\n",
        "\n",
        "df_have_word = pd.DataFrame({'word':have_word})\n",
        "df_token = pd.DataFrame.from_dict(token.word_index.items())\n",
        "\n",
        "df_text_one = pd.DataFrame(text_one[0:],columns= [\"tex\"])\n",
        "df_text_zero = pd.DataFrame(text_zero[0:],columns= [\"tex\"])\n",
        "\n",
        "from collections import Counter\n",
        "count_zero = df_join_text_zero['tex'].str.split().apply(Counter)\n",
        "count_one = df_join_text_one['tex'].str.split().apply(Counter)\n",
        "\n",
        "df_counter_zero = pd.DataFrame.from_dict(count_zero[0], orient='index').reset_index()\n",
        "df_counter_one = pd.DataFrame.from_dict(count_one[0], orient='index').reset_index()\n",
        "df_counter_zero.columns = ['word', 'value']\n",
        "df_counter_one.columns = ['word', 'value']\n",
        "\n",
        "df_token.columns = ['word', 'id']\n",
        "\n",
        "sentiment_zero =df_have_word.merge(df_counter_zero, left_on='word', right_on='word')[['value','word']]\n",
        "sentiment_one = df_have_word.merge(df_counter_one, left_on='word', right_on='word')[['value','word']]\n",
        "\n",
        "#Words found in both positive and negative sentences\n",
        "common = sentiment_one.merge(sentiment_zero,on=['word','word'])\n",
        "common.columns = ['value_one', 'word' , 'value_zero']\n",
        "\n",
        "common[\"max\"] = common[[\"value_one\", \"value_zero\"]].max(axis=1)\n",
        "common[\"min\"] = common[[\"value_one\", \"value_zero\"]].min(axis=1)\n",
        "\n",
        "#Words that were both positive and negative in the sentence were marked with a label column that were more commonly seen in positive or negative sentences.\n",
        "common[\"lable\"] = np.where(common[\"value_one\"] == common[\"max\"], 1, 0)\n",
        "common[\"frequent_negative_sentence\"] = np.where(common[\"lable\"] == 1 , common[\"min\"], common[\"max\"])\n",
        "common[\"frequent_positive_sentence\"] = np.where(common[\"lable\"] == 0 , common[\"min\"], common[\"max\"])\n",
        "\n",
        "common_df_final = common.drop('value_one', 1)\n",
        "common_df_final = common_df_final.drop('value_zero', 1)\n",
        "common_df_final = common_df_final.drop('max', 1)\n",
        "common_df_final = common_df_final.drop('min', 1)\n",
        "\n",
        "#Those words that are seen in positive sentences and not seen in negative sentences\n",
        "just_one = sentiment_one[(~sentiment_one.word.isin(common.word))&(~sentiment_one.word.isin(common.word))]\n",
        "just_one = just_one.assign(lable = 1)\n",
        "just_one = just_one.assign(frequent_negative_sentence = 0)\n",
        "just_one.columns = ['frequent_positive_sentence', 'word' , 'lable','frequent_negative_sentence']\n",
        "\n",
        "#Those words that are seen in negative sentences and not seen in positive sentences\n",
        "just_zero = sentiment_zero[(~sentiment_zero.word.isin(common.word))&(~sentiment_zero.word.isin(common.word))]\n",
        "just_zero = just_zero.assign(lable = 0)\n",
        "just_zero = just_zero.assign(frequent_posoitve_sentence = 0)\n",
        "just_zero.columns = ['frequent_negative_sentence', 'word' , 'lable','frequent_positive_sentence']\n",
        "\n",
        "frames = [common_df_final,just_zero, just_one]\n",
        "result = pd.concat(frames)\n",
        "\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7_Br4DIwSwS"
      },
      "source": [
        "# WI calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CNk31OIwSwS",
        "outputId": "6a3d42e8-d991-439e-c226-4656d48a0eb2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>lable</th>\n",
              "      <th>frequent_negative_sentence</th>\n",
              "      <th>frequent_positive_sentence</th>\n",
              "      <th>WI_positive_word</th>\n",
              "      <th>WI_negative_word</th>\n",
              "      <th>WI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i</td>\n",
              "      <td>0</td>\n",
              "      <td>980</td>\n",
              "      <td>325</td>\n",
              "      <td>None</td>\n",
              "      <td>0.668367</td>\n",
              "      <td>0.668367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to</td>\n",
              "      <td>0</td>\n",
              "      <td>582</td>\n",
              "      <td>261</td>\n",
              "      <td>None</td>\n",
              "      <td>0.551546</td>\n",
              "      <td>0.551546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>0</td>\n",
              "      <td>503</td>\n",
              "      <td>253</td>\n",
              "      <td>None</td>\n",
              "      <td>0.497018</td>\n",
              "      <td>0.497018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a</td>\n",
              "      <td>0</td>\n",
              "      <td>364</td>\n",
              "      <td>163</td>\n",
              "      <td>None</td>\n",
              "      <td>0.552198</td>\n",
              "      <td>0.552198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my</td>\n",
              "      <td>0</td>\n",
              "      <td>404</td>\n",
              "      <td>113</td>\n",
              "      <td>None</td>\n",
              "      <td>0.720297</td>\n",
              "      <td>0.720297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>whithout</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>notebook</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>coincidence</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1406</th>\n",
              "      <td>gsoc</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>cafe</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>None</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2557 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             word  lable  frequent_negative_sentence  \\\n",
              "0               i      0                         980   \n",
              "1              to      0                         582   \n",
              "2             the      0                         503   \n",
              "3               a      0                         364   \n",
              "4              my      0                         404   \n",
              "...           ...    ...                         ...   \n",
              "1403     whithout      1                           0   \n",
              "1404     notebook      1                           0   \n",
              "1405  coincidence      1                           0   \n",
              "1406         gsoc      1                           0   \n",
              "1407         cafe      1                           0   \n",
              "\n",
              "      frequent_positive_sentence WI_positive_word WI_negative_word        WI  \n",
              "0                            325             None         0.668367  0.668367  \n",
              "1                            261             None         0.551546  0.551546  \n",
              "2                            253             None         0.497018  0.497018  \n",
              "3                            163             None         0.552198  0.552198  \n",
              "4                            113             None         0.720297  0.720297  \n",
              "...                          ...              ...              ...       ...  \n",
              "1403                           1              1.0             None  1.000000  \n",
              "1404                           1              1.0             None  1.000000  \n",
              "1405                           1              1.0             None  1.000000  \n",
              "1406                           1              1.0             None  1.000000  \n",
              "1407                           1              1.0             None  1.000000  \n",
              "\n",
              "[2557 rows x 7 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['WI_positive_word'] = np.where(result['lable']== 1 , 1- result['frequent_negative_sentence']/ result['frequent_positive_sentence'], None)\n",
        "result['WI_negative_word'] = np.where(result['lable']== 0 , 1- result['frequent_positive_sentence']/ result['frequent_negative_sentence'], None)\n",
        "result['WI'] = result['WI_positive_word'].fillna(result['WI_negative_word'])\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3FoP8ozwSwS"
      },
      "source": [
        "# Find words in the sentence that are in word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9E6sKvSwSwS"
      },
      "outputs": [],
      "source": [
        "word_have_list= list(result.word)\n",
        "word_have_set = set(word_have_list)\n",
        "result_word_list = result['word'].tolist()\n",
        "result_WI_list = result['WI'].tolist()\n",
        "\n",
        "def word_WI_finder(x):\n",
        "    df_words = set(x.split(' '))\n",
        "    extract_words =  word_set.intersection(df_words)\n",
        "    #index_word = result_word_list.index(extract_words)\n",
        "    \n",
        "    \n",
        "    return (extract_words)\n",
        "\n",
        "\n",
        "word_set = word_have_set\n",
        "\n",
        "df['ddd_WI'] = df.tweet.apply(word_WI_finder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByqhBCuLwSwS"
      },
      "source": [
        "# Calculate weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WI_ww = []\n",
        "for x in df['ddd_WI'][:]:\n",
        "    WI_w = []\n",
        "    for y in x : \n",
        "        ind = result_word_list.index(y)\n",
        "        WI_w.append(result_WI_list[ind])\n",
        "    WI_ww.append(WI_w)\n",
        "    \n",
        "WI_ww_new = [[] if x==[0] else x for x in WI_ww]\n",
        "WI_ww_neww = [[] if x==[0,0] else x for x in WI_ww_new]\n",
        "Weight_sentence = [[float(j)/sum(i) for j in i] for i in WI_ww_neww[:]]\n",
        "#Weight_sentence"
      ],
      "metadata": {
        "id": "FXwk9H04xQVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dr5MqkNwSwS"
      },
      "source": [
        "# Calculate the final sentence vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFkO0WZPwSwT",
        "outputId": "c5f0e0ab-a6da-48d6-e7b6-054a2162edbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_ss = []\n",
        "for i in range(len(df['ddd_WI'][:])):\n",
        "    #print(df['ddd_WI'][x])\n",
        "    vv = []\n",
        "    for j in range(len(df['ddd_WI'][i])):\n",
        "        #print(list(df['ddd_WI'][i])[j])\n",
        "        if len(Weight_sentence[i]) > 1:\n",
        "            v = []\n",
        "            for num in range(d):\n",
        "                vec = Weight_sentence[i][j] * float(glove_vectors.get(list(df['ddd_WI'][i])[j])[num])\n",
        "                v.append(vec)\n",
        "            #print(v)\n",
        "            vv.append(v)\n",
        "        #print(vv)\n",
        "        else:\n",
        "            vv.append(glove_vectors.get(list(df['ddd_WI'][i])[j]).astype(np.float))\n",
        "            \n",
        "    s = [sum(x) for x in zip(*vv)]\n",
        "    sentence_ss.append(s)\n",
        "\n",
        "sentence_ss[0]\n",
        "\n",
        "data = pd.DataFrame(sentence_ss[0:] , columns = list(range(d)))\n",
        "#X_test = data[:]\n",
        "#y_test = df['sentiment'][:]\n",
        "data[:].to_csv('X_test.csv', sep='\\t', encoding='utf-8')\n",
        "df['sentiment'][:].to_csv('y_test.csv', sep='\\t', encoding='utf-8')\n",
        "\n",
        "#d = pd.read_csv(\"s.csv\",sep='\\t')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "okFLPgIUwSwE",
        "MqJIfulAwSwL",
        "AThT9n-ZwSwN",
        "2TJc6F38wSwO",
        "a9c-ETSSwSwQ",
        "KInDZ1ytwSwR",
        "b91cMd8xwSwR",
        "N7_Br4DIwSwS",
        "a3FoP8ozwSwS",
        "ByqhBCuLwSwS",
        "6Dr5MqkNwSwS"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}